# DistilGPT-2 Chatbot ğŸ—£ï¸ğŸ¤–

This repository contains a simple chatbot built using Hugging Face's `distilgpt2`, a distilled version of OpenAIâ€™s GPT-2 model. The project demonstrates how to fine-tune and deploy a lightweight conversational model for custom text generation tasks.

---

## ğŸ“‚ Project Structure

- `chatbot.py` â€“ Core script to generate responses using the `distilgpt2` model.
- `requirements.txt` â€“ Python dependencies required to run the chatbot.
- `notebooks/` â€“ Jupyter notebooks for experimentation, testing, and analysis.

---

## ğŸ’¡ Features

- Lightweight and efficient using `distilgpt2`
- Simple CLI interface for interactive conversations
- Easy to modify or extend for custom use-cases

---

## ğŸ“š Built With

- [Hugging Face Transformers](https://huggingface.co/transformers/)
- Python
- PyTorch
- Streamlit

---

## ğŸ“Œ Notes

- The model is fine-tuned but may generate generic or incoherent responses.
- Ideal as a baseline or prototype for more advanced chatbot pipelines (e.g., RAG, instruction tuning, etc.).

---

## ğŸ‘©ğŸ»â€ğŸ’» Author

Aditi Balaji â€“ [LinkedIn](https://www.linkedin.com/in/aditibalaji) | [GitHub](https://github.com/Aditi-balaji-13)


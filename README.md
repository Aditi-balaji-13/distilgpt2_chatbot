# DistilGPT-2 Chatbot ğŸ—£ï¸ğŸ¤–

This repository contains a simple chatbot built using Hugging Face's `distilgpt2`, a distilled version of OpenAIâ€™s GPT-2 model. The project demonstrates how to fine-tune and deploy a lightweight conversational model for custom text generation tasks.

---

## ğŸ“‚ Project Structure

- `chatbot.py` â€“ Core script to generate responses using the `distilgpt2` model.
- `requirements.txt` â€“ Python dependencies required to run the chatbot.
- `notebooks/` â€“ Jupyter notebooks for experimentation, testing, and analysis.

---

## ğŸš€ How to Run

1. **Clone the Repository**  
'''
git clone https://github.com/Aditi-balaji-13/distilgpt2_chatbot.git
cd distilgpt2_chatbot
'''

2. **Install Requirements**
'''
pip install -r requirements.txt

'''
3. **Run the Chatbot**
'''
python chatbot.py
'''


4. **Interact**  
Enter any message, and get a generated response from the model!

---

## ğŸ’¡ Features

- Lightweight and efficient using `distilgpt2`
- Simple CLI interface for interactive conversations
- Easy to modify or extend for custom use-cases

---

## ğŸ“š Built With

- [Hugging Face Transformers](https://huggingface.co/transformers/)
- Python
- PyTorch

---

## ğŸ“Œ Notes

- The model is not fine-tuned and may generate generic or incoherent responses.
- Ideal as a baseline or prototype for more advanced chatbot pipelines (e.g., RAG, instruction tuning, etc.).

---

## âœ¨ Future Improvements

- Add a front-end interface (e.g., Streamlit)
- Fine-tune on domain-specific data
- Integrate with speech-to-text and text-to-speech APIs

---

## ğŸ‘©ğŸ»â€ğŸ’» Author

Aditi Balaji â€“ [LinkedIn](https://www.linkedin.com/in/aditibalaji) | [GitHub](https://github.com/Aditi-balaji-13)

